{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "12주차 실습.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfvRP8c9oFcn",
        "colab_type": "text"
      },
      "source": [
        "# Dog, cat image classification problem set\n",
        "\n",
        "* 이번 학습에서는 처음부터 끝까지 Dog, cat dataset에 대한 분류 model을 구현합니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bMlMz4EoFco",
        "colab_type": "text"
      },
      "source": [
        "### [CUDA](http://pytorch.org/docs/stable/cuda.html)\n",
        "\n",
        "* cuda를 이용하겠습니다\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghm6aX2UoFcp",
        "colab_type": "code",
        "outputId": "c26a4fef-1d1e-4e88-eb11-b430d17c05c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "#train_on_gpu = False\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbfbVWvIoFcs",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Load the [Data](http://pytorch.org/docs/stable/torchvision/datasets.html)\n",
        "\n",
        "* 아미지를 다운로드 받습니다\n",
        "* 폴더별로\n",
        " - test\n",
        " - train\n",
        " - validation\n",
        "\n",
        " data를 받습니다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-vIYeDvw0X5",
        "colab_type": "code",
        "outputId": "bcf0b6bc-c29b-4753-a28a-0365802bb5ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install googledrivedownloader"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7_ItD54w8ii",
        "colab_type": "code",
        "outputId": "aff3ba0a-33f0-4a78-9170-7cf77a33afc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from os.path import exists\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "import tarfile \n",
        "\n",
        "#if exists(\"./Cat_Dog_data.tgz\"):\n",
        "#    !rm -rf ./Cat_Dog_data.tgz\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1WpY0qpe7yF5C5M52z1BMIzYVpDYiU3OV',\n",
        "                                    dest_path = './Cat_Dog_data.tgz')\n",
        "\n",
        "tf = tarfile.open(\"Cat_Dog_data.tgz\")\n",
        "tf.extractall()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1WpY0qpe7yF5C5M52z1BMIzYVpDYiU3OV into ./Cat_Dog_data.tgz... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNnl0wVnxvLM",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1 [20 points]: \n",
        "\n",
        "* Training, validation, test를 위한 dataloader, transform을 적절하게 준비해주세요\n",
        "* 아래 data 준비하는 코딩을 수행하고, 아래 markdown에 준비한 과정 및 이유를 구체적으로 설명하세요\n",
        "* 아래 답안 작성에 data의 구조에 대해서 설명하세요\n",
        "* 코드에는 주석을 달아주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiZz3-P3ysrV",
        "colab_type": "code",
        "outputId": "f60901fc-5d7a-4800-e9dc-980624b9ef82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Coding\n",
        "train_dir = 'train'       # train 경로 설정\n",
        "test_dir = 'test'         # test 경로 설정\n",
        "valid_size = 0.2          # validation set으로 분할 할 비율\n",
        "batch_size = 20\n",
        "\n",
        "# 적용 변형 \n",
        "train_transforms = transforms.Compose ([                                    # 이미지 리사이즈\n",
        "                                transforms.RandomRotation (30),           \n",
        "                                transforms.RandomResizedCrop (224), \n",
        "                                transforms.RandomHorizontalFlip (),\n",
        "                                transforms.ToTensor (),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])])\n",
        "test_transforms = transforms.Compose ([transforms.Resize (255),        \n",
        "                                      transforms.CenterCrop (224), \n",
        "                                      transforms.ToTensor (),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])])\n",
        "train_data = datasets.ImageFolder (train_dir,   \n",
        "                                    transform = train_transforms)                                        \n",
        "test_data = datasets.ImageFolder (test_dir, \n",
        "                                    transform = test_transforms)\n",
        "\n",
        "num_train = len(train_data)       # train 데이터의 길이\n",
        "indices = list(range(num_train))    # train 데이터 index\n",
        "np.random.shuffle(indices)    # 순서 랜덤으로 바꿈 섞어서 하는 것이 random성에 좋다\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)      # DataLoder에 sampler 넣기\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)      # DataLoder에 sampler 넣기\n",
        "\n",
        "# 데이터 로딩\n",
        "train_loader = torch.utils.data.DataLoader (train_data, batch_size = batch_size,sampler=train_sampler)      # trainloder 생성\n",
        "test_loader = torch.utils.data.DataLoader (test_data, batch_size = batch_size)    # testloder 생성\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size= batch_size,     # validloder 생성\n",
        "    sampler=valid_sampler)\n",
        "\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img): # 이미지를 보기위한 함수 ()\n",
        "    img = img * 0.5 + 0.5  # unnormalize \n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "\n",
        "\n",
        " # obtain one batch of training images\n",
        "dataiter = iter(train_loader) # train_loader에서 데이터를 받음\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display, tensor를 numpy로 바꿈\n",
        "\n",
        "\n",
        "print(images.shape)     # 사이즈 확인\n",
        "\n",
        "\n",
        "classes = ['cat','dog']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 3, 224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4Ad2T8wyDJN",
        "colab_type": "text"
      },
      "source": [
        "**분석 및 설명:**transforms를 통해 데이터를 리사이즈하고 크롭한다. 크롭한 데이터를 이미지폴더에 넣고 train데이터의 길이를 가져와서 리스트에 저장하여 랜덤화 효과를 위해 순서를 섞는다. 섞인 데이터를 다시 dataloder를 통해 sampler에 넣어준다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4bXK3BxoFdE",
        "colab_type": "text"
      },
      "source": [
        "## Problem 2 [20 point]: Define the Network Architecture\n",
        "\n",
        "* 구현하고자하는 network을 작성해주세요\n",
        "* 아래 구현 방법과 이유를 구체적으로 설명하세요\n",
        "* 코드에는 주석을 달아주세요. \n",
        "* 아래 모델을 구체적으로 설명하고, 설정 이유를 작성해주세요\n",
        "* 본 과정에서는 직접 network을 구현하고, transfer learning은 활용하지 않도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaNxUAd4oFdF",
        "colab_type": "code",
        "outputId": "4fd496b1-e568-4f90-be02-df1037fb1a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# 코드 작성\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # convolutional layer (224,224,3 image tensor)\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        # convolutional layer (112*112*32 tensor)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        # convolutional layer (56*56*64 tensor)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        # max pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # linear layer (28*28*128 -> 500)\n",
        "        self.fc1 = nn.Linear(128*28*28, 1000)\n",
        "        # linear layer (500 -> 2)\n",
        "        self.fc2 = nn.Linear(1000, 500)      # 분류하는게 2개라서\n",
        "        self.fc3 = nn.Linear(500, 2)\n",
        "        # dropout layer (p=0.25)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 128*28*28)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x) \n",
        "        # add 1 hidden layer, relu activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # aadd 2 hidden layer, relu activation\n",
        "        x = F.log_softmax(self.fc3(x),dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = Net()\n",
        "print(model)\n",
        "if train_on_gpu:\n",
        "    model.cuda() "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=100352, out_features=1000, bias=True)\n",
            "  (fc2): Linear(in_features=1000, out_features=500, bias=True)\n",
            "  (fc3): Linear(in_features=500, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkP09gUJzKVc",
        "colab_type": "text"
      },
      "source": [
        "**분석 및 설명:** 3개의 컨볼루션 레이어를 만들고 maxpool을 통해 최대값을 뽑아낸다. 최적화과정에서 NLLLoss를 사용하기 때문에 softmax를 사용하였다. 오버피팅을 막기 위해 dropout을 넣고, relu를 적용하여 0이상의 값들은 그대로 출력을 한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xukdFNh3oFdH",
        "colab_type": "text"
      },
      "source": [
        "## Problem 3 [5 point]: Specify Loss Function\n",
        "\n",
        "* Loss 함수와 optimizer를 구현하세요\n",
        "* 선정 이유를 설명하세요\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuczyoY5oFdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 코드\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum = 0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64psu2RjzdrZ",
        "colab_type": "text"
      },
      "source": [
        " **설명:** softmax를 적용시키고 NLLLoss를 사용하면 CrossEntropy랑 같은효과를 낸다하여 NLLLoss를 사용하였다.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcIVfJz9oFdJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Problem 4 [30 point]: Train the Network\n",
        "\n",
        "* training loss와 validation loss를 기록하며 training을 구현하세요\n",
        "* 만약 validation loss가 최소화된 모델을 저장하세요\n",
        "* 코드에는 모두 주석을 포함해주세요\n",
        "* training과정을 설명하고, training 결과를 분석해주세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "fjBJWFIboFdJ",
        "colab_type": "code",
        "outputId": "b3baa0ce-8f46-42da-c1ba-8a31159b9da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# 코드 작성\n",
        "n_epochs = 35\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:        # \n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # 최적화된 변수의 그라데이션을 지움\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: 모델에 입력을 전달하여 예측 출력을 전달한다\n",
        "        output = model(data)\n",
        "        # batch loss 계산\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: 모델 매개변수에 대한 손실 계산\n",
        "        loss.backward()\n",
        "        # 단일 최적화  모델에 입력을 전달하여 예측 출력을 전달한다\n",
        "        output = model(data)\n",
        "        # batch loss 계산\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: 모델 매개변수에 대한 손실 계산\n",
        "        loss.backward()\n",
        "        # 단일 최적화 단계 수행\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:          # train_on\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: 모델에 입력을 전달하여 예측 출력 계산\n",
        "        output = model(data)\n",
        "        # batch loss 계산\n",
        "        loss = criterion(output, target)\n",
        "        # validation loss 평균 업데이트\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "    \n",
        "    # losses 평균 계산\n",
        "    train_loss = train_loss/len(train_loader.sampler)   # train_loader 크기로 나눔\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)   # valid_loader 크기로 나눔\n",
        "        \n",
        "    # traning / validation 상태 출력 \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(       # 결과 출력\n",
        "        epoch, train_loss, valid_loss))\n",
        "    \n",
        "    # 비교하여 validation loss가 낮은것이 저장\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_catdog.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.677333 \tValidation Loss: 0.638312\n",
            "Validation loss decreased (inf --> 0.638312).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.641688 \tValidation Loss: 0.637168\n",
            "Validation loss decreased (0.638312 --> 0.637168).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.616351 \tValidation Loss: 0.595624\n",
            "Validation loss decreased (0.637168 --> 0.595624).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.601661 \tValidation Loss: 0.574195\n",
            "Validation loss decreased (0.595624 --> 0.574195).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.588810 \tValidation Loss: 0.609953\n",
            "Epoch: 6 \tTraining Loss: 0.586397 \tValidation Loss: 0.579349\n",
            "Epoch: 7 \tTraining Loss: 0.577728 \tValidation Loss: 0.557997\n",
            "Validation loss decreased (0.574195 --> 0.557997).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.565429 \tValidation Loss: 0.538615\n",
            "Validation loss decreased (0.557997 --> 0.538615).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.553421 \tValidation Loss: 0.553278\n",
            "Epoch: 10 \tTraining Loss: 0.552695 \tValidation Loss: 0.585091\n",
            "Epoch: 11 \tTraining Loss: 0.545673 \tValidation Loss: 0.526956\n",
            "Validation loss decreased (0.538615 --> 0.526956).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.535988 \tValidation Loss: 0.511548\n",
            "Validation loss decreased (0.526956 --> 0.511548).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 0.525549 \tValidation Loss: 0.509211\n",
            "Validation loss decreased (0.511548 --> 0.509211).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.521178 \tValidation Loss: 0.503099\n",
            "Validation loss decreased (0.509211 --> 0.503099).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.518456 \tValidation Loss: 0.513018\n",
            "Epoch: 16 \tTraining Loss: 0.506929 \tValidation Loss: 0.511921\n",
            "Epoch: 17 \tTraining Loss: 0.507376 \tValidation Loss: 0.476776\n",
            "Validation loss decreased (0.503099 --> 0.476776).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.494192 \tValidation Loss: 0.504005\n",
            "Epoch: 19 \tTraining Loss: 0.494092 \tValidation Loss: 0.492071\n",
            "Epoch: 20 \tTraining Loss: 0.486308 \tValidation Loss: 0.490323\n",
            "Epoch: 21 \tTraining Loss: 0.479295 \tValidation Loss: 0.478456\n",
            "Epoch: 22 \tTraining Loss: 0.473587 \tValidation Loss: 0.476895\n",
            "Epoch: 23 \tTraining Loss: 0.471915 \tValidation Loss: 0.462580\n",
            "Validation loss decreased (0.476776 --> 0.462580).  Saving model ...\n",
            "Epoch: 24 \tTraining Loss: 0.470045 \tValidation Loss: 0.450036\n",
            "Validation loss decreased (0.462580 --> 0.450036).  Saving model ...\n",
            "Epoch: 25 \tTraining Loss: 0.464246 \tValidation Loss: 0.459551\n",
            "Epoch: 26 \tTraining Loss: 0.459574 \tValidation Loss: 0.450904\n",
            "Epoch: 27 \tTraining Loss: 0.451414 \tValidation Loss: 0.430679\n",
            "Validation loss decreased (0.450036 --> 0.430679).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 0.448424 \tValidation Loss: 0.505489\n",
            "Epoch: 29 \tTraining Loss: 0.449018 \tValidation Loss: 0.458135\n",
            "Epoch: 30 \tTraining Loss: 0.437536 \tValidation Loss: 0.435591\n",
            "Epoch: 31 \tTraining Loss: 0.440430 \tValidation Loss: 0.434326\n",
            "Epoch: 32 \tTraining Loss: 0.437586 \tValidation Loss: 0.410018\n",
            "Validation loss decreased (0.430679 --> 0.410018).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 0.434044 \tValidation Loss: 0.415032\n",
            "Epoch: 34 \tTraining Loss: 0.434700 \tValidation Loss: 0.461475\n",
            "Epoch: 35 \tTraining Loss: 0.431978 \tValidation Loss: 0.412323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkF7i-6Y0EAF",
        "colab_type": "text"
      },
      "source": [
        "**분석 및 설명:**Epoch 수만큼 dataset을 사용, 앞서 설계했던 네트워크를 통해 training을 수행한다. 이후 validation loss를 계산하여 출력한다. validation loss를 비교하여 낮은것이 저장된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqX__Ju3oFdL",
        "colab_type": "text"
      },
      "source": [
        "## Problem 5 [5 point] Validation Loss가 최소화된 Model가져오기\n",
        "\n",
        "* 최소 validation loss를 갖는 model을 불러옵니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LF9TzTWoFdM",
        "colab_type": "code",
        "outputId": "5e867f0a-aa94-4664-d9d6-adab634e8d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 코드 작성\n",
        "model.load_state_dict(torch.load('model_catdog.pt'))        # 저장된 validation loss 가져오기"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLg9vQC2oFdR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Problem 6 [20point]: Test the Trained Network\n",
        "\n",
        "* Test set을 활용하여 성능 검증\n",
        "* Accuracy (분류 성공률)와 test loss를 출력하세요\n",
        "* 코드에는 주석을 달아주세요\n",
        "* 아래 test 결과에 대해서 간단하게 설명/분석 해주세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh_Pu57ToFdR",
        "colab_type": "code",
        "outputId": "e332fd2c-94ad-4894-8c40-940365ab739b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# 코드 작성\n",
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for data, target in test_loader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # batch loss 계산\n",
        "    loss = criterion(output, target)\n",
        "    # test loss 업데이트\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))       # target의 데이터를 prediction화 해서 같은지 비교\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())   # train_on gpu가 아니면 correct_tensor를 numpy화 맞으면 cpu()로 옮긴후 numpy\n",
        "    # calculate test accuracy for each object class     # 각 클래스 정확도 계산\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "        \n",
        "        \n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)      # test_loss를 testloader길이로 나눔\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(2):\n",
        "    if class_total[i] > 0:      # class_total index i가 0보다 크면\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))    # 확률 출력\n",
        "           \n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))    # 0보다 작은경우 문구출력\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))          # class 토탈의 합으로 나눔\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.295598\n",
            "\n",
            "Test Accuracy of   cat: 83% (1046/1250)\n",
            "Test Accuracy of   dog: 90% (1136/1250)\n",
            "\n",
            "Test Accuracy (Overall): 87% (2182/2500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywjwrJ691eL_",
        "colab_type": "text"
      },
      "source": [
        "**분석 및 설명:**validation에서는 전체 네트워크를 사용하기 때문에 dropout을 비활성화 시킬 필요가 있다. 이를 위해 model.eval를 실행을 한다. loss가 최소화된 model을 가져와서 분류가 성공한 비율을 찾는 과정이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkhG9np03CEm",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Problem 7: 전체적인 총평\n",
        "\n",
        "* Data준비, Training과 validation 과정에서 성능 개선을 위해서 수행한 과정과 성공/실패 이유를 분석해주세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCr5fHLi3UIQ",
        "colab_type": "text"
      },
      "source": [
        "**설명:**dropout을 바꿔보았고, nomalize를 추가하였다. convolution layer 역시 조절을 해보았지만 크게 유의미한 결과는 나타나지 않았다. 다만 nomalize를 추가했을때 차이가 5%정도로 나타났다. batch_size는 개선이 조금 보였다. 처음에 32로 설정했을때 계속 index 오류가 떠서 원인은 모르겠지만 사이즈를 줄이니 오류는 뜨지 않았다. 네트워크을 잘못 짜서 그런지 loss가 너무 조금씩 줄어들었다."
      ]
    }
  ]
}